{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2zyTtrgVwD8"
   },
   "source": [
    "# Building Neural Networks with TensorFlow 2.x Core API\n",
    "\n",
    "In this tutorial, we'll build a multi-layer neural network using TensorFlow 2.x **Core API**.\n",
    "We'll define a **custom Dense layer**, explore **different activation functions**, and train the model on the **MNIST dataset**.\n",
    "\n",
    "## **What You Will Learn**\n",
    "- How to create a **custom Dense layer** using `tf.Module`.\n",
    "- How to **manually implement forward propagation**.\n",
    "- How **batches and epochs** work in training.\n",
    "- How to experiment with **different activation functions**.\n",
    "- How to train and evaluate a **multi-layer neural network** on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LGKMoF8VwD-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHhLuNpUVwD_"
   },
   "source": [
    "### Load and Prepare MNIST Dataset\n",
    "The MNIST dataset contains **70,000 grayscale images** of handwritten digits (0-9).\n",
    "We split into **60,000 training** and **10,000 test** images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0q9C5kyVwEA"
   },
   "outputs": [],
   "source": [
    "(train_data, test_data), info = tfds.load('mnist', split=['train', 'test'], as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Count training dataset size\n",
    "train_size = info.splits['train'].num_examples\n",
    "print(f\"Number of elements in train_data: {train_size}\")\n",
    "\n",
    "test_size = info.splits['test'].num_examples\n",
    "print(f\"Number of elements in test_data: {test_size}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDjHhjU8WNM2",
    "outputId": "ead3bb12-7502-4f25-cd92-47ef5e1dedce"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of elements in train_data: 60000\n",
      "Number of elements in test_data: 10000\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruLGJzZxVwEA"
   },
   "source": [
    "## **Understanding `map`, `batch`, and `shuffle` in tf.data**\n",
    "\n",
    "- **`map`**: Applies a function to transform the dataset (e.g., normalize images).\n",
    "- **`shuffle`**: Randomizes dataset order to prevent overfitting (uses a buffer size of `10000` means the dataset maintains 10,000 elements in memory, randomly shuffling before producing new batches..\n",
    "- **`batch`**: Groups samples into mini-batches, improving performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1lNXRdZVwEA"
   },
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "    return tf.cast(image, tf.float32) / 255.0, tf.cast(label, tf.int32)\n",
    "\n",
    "# Apply mapping function\n",
    "train_data = train_data.map(normalize_img)\n",
    "test_data = test_data.map(normalize_img)\n",
    "\n",
    "# Shuffle training data\n",
    "# The buffer size of 10000 means the dataset maintains 10,000 elements in memory,\n",
    "# randomly shuffling before producing new batches.\n",
    "train_data = train_data.shuffle(10000)\n",
    "\n",
    "# Batch the dataset with drop_remainder=True to ensure all batches have the same size\n",
    "train_data = train_data.batch(128, drop_remainder=True)\n",
    "test_data = test_data.batch(128, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Print dataset size and shape\n",
    "# Compute number of batches using dataset cardinality (faster)\n",
    "train_size = train_data.cardinality().numpy()\n",
    "print(f\"Number of batches in train_data: {train_size}\")\n",
    "\n",
    "for batch in train_data.take(1):\n",
    "    images, labels = batch\n",
    "    print(f\"Shape of one batch of images: {images.shape}\")\n",
    "    print(f\"Shape of one batch of labels: {labels.shape}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5UpOPGGYY02m",
    "outputId": "8557c2c2-d03b-48fe-f84b-0f9d21fd7882"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of batches in train_data: 468\n",
      "Shape of one batch of images: (128, 28, 28, 1)\n",
      "Shape of one batch of labels: (128,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOr0cT3jVwEB"
   },
   "source": [
    "## **Creating a Custom Dense Layer**\n",
    "Instead of using `tf.keras.layers.Dense`, we'll **create our own Dense layer** using `tf.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ESidCy6fVwEB"
   },
   "outputs": [],
   "source": [
    "class CustomDense(tf.Module):\n",
    "    def __init__(self, input_dim, output_dim, activation=tf.identity, name=None):\n",
    "        super().__init__(name=name)\n",
    "        # Small vals => To Prevent Exploding/Vanishing Gradients\n",
    "        self.W = tf.Variable(tf.random.normal([input_dim, output_dim], stddev=0.1), name='weights')\n",
    "        self.b = tf.Variable(tf.zeros([output_dim]), name='bias')\n",
    "        self.activation = activation\n",
    "\n",
    "    def __call__(self, x):\n",
    "        z = tf.matmul(x, self.W) + self.b\n",
    "        return self.activation(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDQI11bYVwEB"
   },
   "source": [
    "## **Step 2: Building a Multi-Layer Neural Network (MLP)**\n",
    "MLP => Multi-Layer Perceptron\n",
    "We define a **2-layer MLP**, but we show how to easily increase the number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tlq8wk9jVwEB"
   },
   "outputs": [],
   "source": [
    "class MultiLayerNN(tf.Module):\n",
    "    def __init__(self, input_dim, hidden_units=[128, 64], output_dim=10, activation=tf.nn.relu):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.hidden1 = CustomDense(input_dim, hidden_units[0], activation=activation)\n",
    "        self.hidden2 = CustomDense(hidden_units[0], hidden_units[1], activation=activation)\n",
    "        self.output_layer = CustomDense(hidden_units[1], output_dim, activation=tf.nn.softmax)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcPV5BtJVwEB"
   },
   "source": [
    "To add more hidden layers, simply add more `CustomDense` layers in `__init__`.\n",
    "\n",
    "## **Step 3: Training the Neural Network**\n",
    "- **Sparse categorical cross-entropy** is used when labels are **integers** (e.g., `0-9` for MNIST).\n",
    "- If labels are **one-hot encoded**, we would use `categorical_crossentropy` instead.\n",
    "- Alternatives:\n",
    " - **MSE (Mean Squared Error)**: Not ideal for classification.\n",
    " - **Binary Cross-Entropy**: Used for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jq_pLAYXVwEB"
   },
   "outputs": [],
   "source": [
    "# Why Use from_logits=True?\n",
    "# ✅ Better numerical stability (avoids floating-point issues).\n",
    "# ✅ Better gradient flow (softmax + crossentropy computed together).\n",
    "# ✅ ***No need to apply softmax in the output layer***.\n",
    "# self.output_layer = CustomDense(hidden_units[1], output_dim, activation=None)\n",
    "# ⚠ Note: If from_logits=True, y_pred should be logits (before softmax).\n",
    "\n",
    "def compute_loss(y_pred, y_true):\n",
    "    return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Understanding Batches and Epochs**\n",
    "- **Batch Size:** The number of training samples processed **at once** before updating model parameters.\n",
    "- **Epoch:** A full pass over the entire training dataset.\n",
    "\n",
    "### **Explanation: Predictions and Accuracy Computation**\n",
    "- `tf.argmax(y_pred, axis=1)`: Gets the class with the highest probability.\n",
    "- `tf.equal(predictions, y_batch)`: Checks if predictions match true labels.\n",
    "- `tf.reduce_mean(tf.cast(correct_preds, tf.float32))`: Computes accuracy.\n",
    "\n",
    "#### Adam\n",
    "Adam (Adaptive Moment Estimation) is an advanced optimization algorithm that combines the benefits of Momentum and RMSprop.\n",
    "\n",
    "- Pros ✅\n",
    "- ✔ Adaptive Learning Rate: Automatically adjusts learning rate for each parameter.\n",
    "- ✔ Momentum-Based: Accelerates convergence.\n",
    "- ✔ Handles Sparse Gradients Well: Works great for deep networks like CNNs and RNNs.\n",
    "- ✔ Less Hyperparameter Tuning: Default values work well for many tasks.\n",
    "\n",
    "- Cons ❌\n",
    "- ✖ Uses More Memory: Stores momentum and squared gradients.\n",
    "- ✖ Can Overshoot Optimum: May struggle with local minima in some cases.\n",
    "\n",
    "- Best for:\n",
    "- ✅ Deep learning models (CNNs, RNNs, Transformers).\n",
    "- ✅ Large-scale datasets (like ImageNet, NLP tasks).\n",
    "- ✅ Sparse gradient problems (e.g., NLP embeddings).\n",
    "\n",
    "#### SGD\n",
    "Stochastic Gradient Descent: is the simplest optimization method.\n",
    "\n",
    "- Pros ✅\n",
    "- ✔ Lightweight: Uses minimal memory.\n",
    "- ✔ Can Generalize Better: Works well for convex problems (e.g., linear/logistic regression).\n",
    "- ✔ More Control: Users can manually tune the learning rate.\n",
    " \n",
    "- Cons ❌\n",
    "- ✖ Slow Convergence: Can get stuck in plateaus.\n",
    "- ✖ Learning Rate Sensitivity: Requires careful tuning.\n",
    "- ✖ Doesn’t Handle Curvature Well: No adaptive learning rate.\n",
    "\n",
    "- Best for:\n",
    "- ✅ Shallow networks (MLPs, logistic regression, linear regression).\n",
    "- ✅ Small datasets.\n",
    "- ✅ Situations where we want to prevent overfitting (better generalization).\n",
    "\n"
   ],
   "metadata": {
    "id": "Oti5t7tfczqg"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "soYYqCmBVwEB"
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataset, learning_rate=0.01, epochs=5):\n",
    "    optimizer = tf.optimizers.Adam(learning_rate)  # Alternative: tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for x_batch, y_batch in dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = model(x_batch)\n",
    "                loss = compute_loss(y_pred, y_batch)\n",
    "\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "            predictions = tf.argmax(y_pred, axis=1, output_type=tf.int32)\n",
    "            accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, y_batch), tf.float32)).numpy()\n",
    "\n",
    "            total_loss += loss.numpy()\n",
    "            total_acc += accuracy\n",
    "            num_batches += 1\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / num_batches:.4f}, Accuracy: {total_acc / num_batches:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOkQqp_aVwEB"
   },
   "source": [
    "## **Step 4: Evaluating the Model**\n",
    "Let's check how well the trained model performs on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FT2vCfjtVwEB"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset):\n",
    "    total_acc = 0\n",
    "    num_batches = 0\n",
    "    for x_batch, y_batch in dataset:\n",
    "        y_pred = model(x_batch)\n",
    "        predictions = tf.argmax(y_pred, axis=1, output_type=tf.int32) # axis=1 is row wize\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, y_batch), tf.float32)).numpy()\n",
    "        total_acc += accuracy\n",
    "        num_batches += 1\n",
    "    return total_acc / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Initialize and Train Model\n",
    "# **Hyperparameter Options:**\n",
    "- **Activation Functions**\n",
    "- **Hidden Units**\n",
    "- **Learning Rates**\n",
    "- **Epochs**"
   ],
   "metadata": {
    "id": "U8QJTdL9de6_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a-VyaIFBVwEC",
    "outputId": "7742c05f-6fba-49a2-8b34-6c17e1fab6b8"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1, Loss: 1.5804, Accuracy: 0.8944\n",
      "Epoch 2, Loss: 1.5092, Accuracy: 0.9558\n",
      "Epoch 3, Loss: 1.4968, Accuracy: 0.9669\n",
      "Epoch 4, Loss: 1.4900, Accuracy: 0.9730\n",
      "Epoch 5, Loss: 1.4851, Accuracy: 0.9778\n",
      "Test Accuracy: 0.9732\n"
     ]
    }
   ],
   "source": [
    "mlp_model = MultiLayerNN(input_dim=784, hidden_units=[256, 128], output_dim=10, activation=tf.nn.relu)\n",
    "train_model(mlp_model, train_data, learning_rate=0.001, epochs=5)\n",
    "test_accuracy = evaluate_model(mlp_model, test_data)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Exercise: #\n",
    "Change the number of hidden units, the learning rate, and the number of epochs and observe and report the impact on performance.\n",
    "- **Activation Functions:** `tf.nn.relu`, `tf.nn.sigmoid`, `tf.nn.tanh`\n",
    "- **Hidden Units:** `[256, 128], [512, 128], [128, 64]`\n",
    "- **Learning Rates:** `[0.001, 0.01, 0.0001]`\n",
    "- **Epochs:** `[5, 10, 20]`\n",
    "\n"
   ],
   "metadata": {
    "id": "HzcUYmpYdz01"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "W-ChSPDGeeuQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwJf0DeeVwEC"
   },
   "source": [
    "## **Conclusion**\n",
    "🎉 We've successfully built a neural network using TensorFlow 2.x **Core API**, implemented a **custom Dense layer**, trained on MNIST, and explored **different activation functions**.\n",
    "\n",
    "🚀 **Next Steps:** Try changing the activation functions (`sigmoid`, `tanh`), or modify the number of layers and neurons!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
